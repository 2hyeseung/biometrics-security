{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1871080_이혜승_홍채_2차_코드.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRbRnTB7l1LA"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD4EAIzPnKrg"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from numpy import load\n",
        "from numpy import expand_dims\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "from keras.models import load_model\n",
        "from numpy import load\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSWPO2FMm4Ez"
      },
      "source": [
        "X_train=[]\n",
        "y_train=[]\n",
        "X_val=[]\n",
        "y_val=[]\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/생체인증보안/03_iris_training')\n",
        "for i in os.listdir():\n",
        "    img = cv2.imread(i)   \n",
        "    img = cv2.resize(img,(160,160))\n",
        "    X_val.append(img)\n",
        "    y_val.append(i[0:3])\n",
        "    X_train.append(img)\n",
        "    y_train.append(i[0:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsmK3YRsojNc"
      },
      "source": [
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/생체인증보안/03_iris_test')\n",
        "for i in os.listdir():\n",
        "    img = cv2.imread(i)   \n",
        "    img = cv2.resize(img,(160,160))\n",
        "    X_test.append(img)\n",
        "    y_test.append(i.rstrip('.png'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr8U0tT6mzBT"
      },
      "source": [
        "print (\"Shape of an image in X_train: \", X_train[0].shape)\n",
        "print (\"Shape of an image in X_val: \", X_val[0].shape)\n",
        "print (\"Shape of an image in X_test: \", X_test[0].shape)\n",
        "print(\"Total categories: \", len(np.unique(y_train)))\n",
        "print(\"Total categories: \", len(np.unique(y_val)))\n",
        "print(\"Total categories: \", len(np.unique(y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2d5m_OvnaSC"
      },
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=64)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "y_val = le.fit_transform(y_val)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes=64)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "X_val=np.array(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0_PQXUqswa0"
      },
      "source": [
        "from numpy import asarray\n",
        "print(\"X_train Shape: \", X_train.shape)\n",
        "print(\"y_train Shape: \", y_train.shape)\n",
        "\n",
        "print(\"X_val Shape: \", X_val.shape)\n",
        "print(\"y_val Shape: \", y_val.shape)\n",
        "\n",
        "print(\"y_test Shape: \", X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSLczML2BsHj"
      },
      "source": [
        "from imgaug import augmenters as iaa\n",
        "\n",
        "seq = iaa.Sequential([\n",
        "    # blur images with a sigma of 0 to 0.5\n",
        "    iaa.GaussianBlur(sigma=(0, 0.5)),\n",
        "    iaa.Affine(\n",
        "        # scale images to 90-110% of their size, individually per axis\n",
        "        scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)},\n",
        "        # translate by -10 to +10 percent (per axis)\n",
        "        translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
        "        # if mode is constant, use a cval between 0 and 255\n",
        "        cval=255,\n",
        "    )\n",
        "], random_order=True)\n",
        "\n",
        "x=X_train\n",
        "y=y_train\n",
        "aug=40\n",
        "\n",
        "for i in range(aug):\n",
        "\tx_aug=seq.augment_images(x)\n",
        "\tX_train=np.append(X_train, x_aug, axis=0)\n",
        "\ty_train=np.append(y_train, y, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1RtOAETqRd5"
      },
      "source": [
        "from numpy import asarray\n",
        "print(\"X_train Shape: \", X_train.shape)\n",
        "print(\"y_train Shape: \", y_train.shape)\n",
        "\n",
        "print(\"X_val Shape: \", X_val.shape)\n",
        "print(\"y_val Shape: \", y_val.shape)\n",
        "\n",
        "print(\"y_test Shape: \", X_test.shape)\n",
        "\n",
        "# print(\"X_train_aug Shape: \", X_train_aug.shape)\n",
        "# print(\"y_train_aug Shape: \", y_train_aug.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSrP0XYQZJV6"
      },
      "source": [
        "from keras import backend as K\n",
        "def recall(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
        "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
        "\n",
        "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return recall\n",
        "\n",
        "\n",
        "def precision(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
        "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
        "\n",
        "    # Precision = (True Positive) / (True Positive + False Positive)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return precision\n",
        "\n",
        "\n",
        "def f1score(y_target, y_pred):\n",
        "    _recall = recall(y_target, y_pred)\n",
        "    _precision = precision(y_target, y_pred)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
        "    \n",
        "    # return a single tensor value\n",
        "    return _f1score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv1-Ovqs7hGw"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D, Dropout, Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "\n",
        "# 기존 코드 수정\n",
        "def get_model():\n",
        "    m1=Sequential()\n",
        "    m1.add(BatchNormalization(input_shape = (160,160,3)))\n",
        "    m1.add(Convolution2D(32, (3,3), activation ='relu', input_shape = (160, 160, 3))) \n",
        "    m1.add(MaxPooling2D(pool_size=2))\n",
        "    m1.add(Dropout(0.1))\n",
        "\n",
        "    m1.add(Convolution2D(filters=64,kernel_size=3,padding='same',activation='relu'))\n",
        "    m1.add(MaxPooling2D(pool_size=2))\n",
        "    m1.add(Dropout(0.1))\n",
        "\n",
        "    m1.add(Convolution2D(filters=128,kernel_size=3,padding='same',activation='relu'))\n",
        "    m1.add(MaxPooling2D(pool_size=2))\n",
        "    m1.add(Dropout(0.1))\n",
        "\n",
        "    m1.add(Convolution2D(filters=256,kernel_size=3,padding='same',activation='relu'))\n",
        "    m1.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "    m1.add(Flatten()) \n",
        "    m1.add(Dense(units=160,activation = 'relu'))\n",
        "    m1.add(BatchNormalization())\n",
        "    m1.add(Dense(units = 64, activation = 'softmax'))\n",
        "    m1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', precision, recall, f1score])  \n",
        "    return m1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW5c-2MD7jCi"
      },
      "source": [
        "# from sklearn.model_selection import KFold \n",
        "# from sklearn.model_selection import cross_val_score\n",
        "\n",
        "##cross validation\n",
        "# # kfold = KFold(n_splits=5, random_state = 2021, shuffle=True)\n",
        "# # acc_list = [] \n",
        "# # for train_index, val_index in kfold.split(X_train):\n",
        "# #     x_train_fold, x_val_fold = X_train[train_index], X_train[val_index]\n",
        "# #     y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "# #     model = get_model()\n",
        "# #     model.fit(x_train_fold, y_train_fold, epochs = 20, validation_data = (x_val_fold, y_val_fold))\n",
        "\n",
        "# #     _, acc = model.evaluate(X_val,y_val)\n",
        "# #     acc_list.append(round(acc,4))\n",
        "\n",
        "model=get_model()\n",
        "model.summary()\n",
        "model.fit(X_train, y_train, epochs=100, validation_data=(X_val,y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0NCfyJuM1ot"
      },
      "source": [
        "# # 새로운 코드\n",
        "# # Resnet50\n",
        "# from keras.applications.resnet_v2 import ResNet50V2, ResNet101V2, ResNet152V2, preprocess_input, decode_predictions\n",
        "# from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "# from keras.applications.resnet50 import ResNet50\n",
        "# from keras.layers import Dense, Input, Activation\n",
        "# from keras.models import Model\n",
        "# from keras.callbacks import EarlyStopping\n",
        "# from keras import optimizers\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "# from keras.layers.normalization import BatchNormalization\n",
        "# from keras.layers import MaxPooling2D\n",
        "\n",
        "# input = Input(shape=(160, 160, 3))\n",
        "\n",
        "# def get_model():\n",
        "#     resnet=ResNet50V2(include_top=False, weights='imagenet', input_shape=input)\n",
        "#     m2=resnet.output\n",
        "#     m2=MaxPooling2D(pool_size=(2,2))(m2)\n",
        "#     m2=Flatten()(m2)\n",
        "#     m2=Dropout(0.5)(m2)\n",
        "#     m2=Dense(512,activation='relu', input_dim=input)(m2)\n",
        "#     m2=BatchNormalization()(m2)\n",
        "#     m2=Dense(256,activation='relu')(m2)\n",
        "#     m2=BatchNormalization()(m2)\n",
        "#     m2=Dense(64, activation='softmax')(m2)\n",
        "#     m2=Model(inputs=resnet.input, outputs=m2)\n",
        "#     m2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', precision, recall, f1score])  \n",
        "#     return m2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWGPwmZUUbxa"
      },
      "source": [
        "# # 새로운 코드 - RESNET50\n",
        "\n",
        "# from keras.applications.resnet50 import ResNet50\n",
        "# from keras.layers import Dense, Input, Activation\n",
        "# from keras.models import Model\n",
        "# from keras.callbacks import EarlyStopping\n",
        "# from keras import optimizers\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "# from keras.layers.normalization import BatchNormalization\n",
        "# from keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "# input = Input(shape=(160, 160, 3))\n",
        "# model = ResNet50V2(input_tensor=input, include_top=False, weights='imagenet', pooling='max')\n",
        "\n",
        "# for layer in model.layers[:44]:\n",
        "#     layer.trainable=False\n",
        "\n",
        "# x = model.output\n",
        "# x = BatchNormalization()(x)\n",
        "# # x = GlobalAveragePooling2D()(x)\n",
        "# x = Dropout(0.2)(x)\n",
        "# x = Dense(512, activation='relu')(x)\n",
        "# x = BatchNormalization()(x)\n",
        "# x = Dropout(0.2)(x)\n",
        "# x = Dense(64, activation='softmax', name='softmax')(x)\n",
        "# model = Model(model.input, x)\n",
        "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', precision, recall, f1score])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgzpO_SgRfFA"
      },
      "source": [
        "# # model=get_model()\n",
        "# # model.summary()\n",
        "# model.fit(X_train, y_train, epochs=100, validation_data=(X_val,y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewzmP4znHSSx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLaDFGrVZK_D"
      },
      "source": [
        "predicted = model.predict(X_test, batch_size=128)\n",
        "predicted = np.argmax(np.round(predicted),axis=1)\n",
        "predicted=predicted.tolist()\n",
        "for i in range(256):\n",
        "    predicted[i]+=1\n",
        "print(\"Prediction :\",predicted)\n",
        "\n",
        "print(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Suyz8JdcESz5"
      },
      "source": [
        "y_res=[]\n",
        "y_res = map(int, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp-NsoYeae7r"
      },
      "source": [
        "result = { name:value for name, value in zip(y_res, predicted) }\n",
        "result = sorted(result.items())\n",
        "\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QTh3s_waWIY"
      },
      "source": [
        "import csv\n",
        "with open('1871080_이혜승_홍채_2차_답안.csv', 'w', newline='') as file:\n",
        "    mywriter = csv.writer(file, delimiter=',')\n",
        "    mywriter.writerows(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}