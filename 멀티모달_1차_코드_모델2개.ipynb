{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1871080_이혜승_멀티모달_1차_코드_모델2개.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRbRnTB7l1LA"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD4EAIzPnKrg"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from numpy import load\n",
        "from numpy import expand_dims\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "from keras.models import load_model\n",
        "from numpy import load\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSWPO2FMm4Ez"
      },
      "source": [
        "#train dataset\n",
        "\n",
        "X_train_face=[]\n",
        "X_train_iris=[]\n",
        "y_train_face=[]\n",
        "y_train_iris=[]\n",
        "\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/생체인증보안/04_multimodal_training')\n",
        "for i in os.listdir():\n",
        "    img = cv2.imread(i)   \n",
        "    img = cv2.resize(img,(160,160))\n",
        "    if(i[4:8]==\"face\"):\n",
        "        X_train_face.append(img)\n",
        "        y_train_face.append(i[0:3])\n",
        "    if(i[4:8]==\"iris\"):\n",
        "        X_train_iris.append(img)\n",
        "        y_train_iris.append(i[0:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsmK3YRsojNc"
      },
      "source": [
        "#test dataset\n",
        "\n",
        "X_test_face = []\n",
        "X_test_iris = []\n",
        "y_test = []\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/생체인증보안/04_multimodal_test')\n",
        "for i in os.listdir():\n",
        "    img = cv2.imread(i)   \n",
        "    img = cv2.resize(img,(160,160))\n",
        "    if(i[3:7]==\"face\" or i[2:6]==\"face\"):\n",
        "        X_test_face.append(img)\n",
        "    if(i[3:7]==\"iris\" or i[2:6]==\"iris\"):\n",
        "        X_test_iris.append(img)\n",
        "        y_test.append(i.rstrip(\"_iris.png\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a8IN7CRk0WI"
      },
      "source": [
        "#val dataset\n",
        "\n",
        "X_val_face=[]\n",
        "X_val_iris=[]\n",
        "y_val=[]\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/생체인증보안/04_multimodal_training')\n",
        "for i in os.listdir():\n",
        "    img = cv2.imread(i)   \n",
        "    img = cv2.resize(img,(160,160))\n",
        "    if(i[4:8]==\"face\"):\n",
        "        X_val_face.append(img)\n",
        "    if(i[4:8]==\"iris\"):\n",
        "        X_val_iris.append(img)\n",
        "        y_val.append(i[0:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr8U0tT6mzBT"
      },
      "source": [
        "print (\"Shape of an image in X_train_face: \", X_train_face[0].shape)\n",
        "print (\"Shape of an image in X_train_iris: \", X_train_iris[0].shape)\n",
        "\n",
        "print (\"Shape of an image in X_train_face: \", X_val_face[0].shape)\n",
        "print (\"Shape of an image in X_train_iris: \", X_val_iris[0].shape)\n",
        "\n",
        "print (\"Shape of an image in X_train_face: \", X_test_face[0].shape)\n",
        "print (\"Shape of an image in X_train_iris: \", X_test_iris[0].shape)\n",
        "\n",
        "print(\"Total categories: \", len(np.unique(y_train_iris)))\n",
        "print(\"Total categories: \", len(np.unique(y_train_face)))\n",
        "print(\"Total categories: \", len(np.unique(y_val)))\n",
        "print(\"Total categories: \", len(np.unique(y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2d5m_OvnaSC"
      },
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "y_train_iris = le.fit_transform(y_train_iris)\n",
        "y_train_iris = tf.keras.utils.to_categorical(y_train_iris, num_classes=64)\n",
        "y_train_iris = np.array(y_train_iris)\n",
        "y_train_face = le.fit_transform(y_train_face)\n",
        "y_train_face = tf.keras.utils.to_categorical(y_train_face, num_classes=64)\n",
        "y_train_face = np.array(y_train_face)\n",
        "\n",
        "y_val = le.fit_transform(y_val)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes=64)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "X_train_iris = np.array(X_train_iris)\n",
        "X_test_iris = np.array(X_test_iris)\n",
        "X_val_iris = np.array(X_val_iris)\n",
        "X_train_face = np.array(X_train_face)\n",
        "X_test_face = np.array(X_test_face)\n",
        "X_val_face = np.array(X_val_face)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0_PQXUqswa0"
      },
      "source": [
        "from numpy import asarray\n",
        "print(\"X_train_iris Shape: \", X_train_iris.shape)\n",
        "print(\"X_train_face Shape: \", X_train_face.shape)\n",
        "print(\"y_train_iris Shape: \", y_train_iris.shape)\n",
        "print(\"y_train_face Shape: \", y_train_face.shape)\n",
        "\n",
        "print(\"X_val_iris Shape: \", X_val_iris.shape)\n",
        "print(\"X_val_face Shape: \", X_val_face.shape)\n",
        "print(\"y_val Shape: \", y_val.shape)\n",
        "\n",
        "print(\"X_test_iris Shape: \", X_test_iris.shape)\n",
        "print(\"X_test_face Shape: \", X_test_face.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSLczML2BsHj"
      },
      "source": [
        "from imgaug import augmenters as iaa\n",
        "\n",
        "seq = iaa.Sequential([\n",
        "    # blur images with a sigma of 0 to 0.5\n",
        "    iaa.GaussianBlur(sigma=(0, 0.5)),\n",
        "    iaa.Affine(\n",
        "        # scale images to 90-110% of their size, individually per axis\n",
        "        scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)},\n",
        "        # translate by -10 to +10 percent (per axis)\n",
        "        translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
        "        # if mode is constant, use a cval between 0 and 255\n",
        "        cval=255,\n",
        "    )\n",
        "], random_order=True)\n",
        "\n",
        "x=X_train_iris\n",
        "y=y_train_iris\n",
        "aug=20\n",
        "\n",
        "for i in range(aug):\n",
        "\tx_aug=seq.augment_images(x)\n",
        "\tX_train_iris=np.append(X_train_iris, x_aug, axis=0)\n",
        "\ty_train_iris=np.append(y_train_iris, y, axis=0)\n",
        " \n",
        "\n",
        "x=X_train_face\n",
        "y=y_train_face\n",
        "aug=20\n",
        "\n",
        "for i in range(aug):\n",
        "\tx_aug=seq.augment_images(x)\n",
        "\tX_train_face=np.append(X_train_face, x_aug, axis=0)\n",
        "\ty_train_face=np.append(y_train_face, y, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1RtOAETqRd5"
      },
      "source": [
        "print(\"X_train_iris Shape: \", X_train_iris.shape)\n",
        "print(\"X_train_face Shape: \", X_train_face.shape)\n",
        "print(\"y_train_iris Shape: \", y_train_iris.shape)\n",
        "print(\"y_train_face Shape: \", y_train_face.shape)\n",
        "\n",
        "print(\"X_val_iris Shape: \", X_val_iris.shape)\n",
        "print(\"X_val_face Shape: \", X_val_face.shape)\n",
        "print(\"y_val Shape: \", y_val.shape)\n",
        "\n",
        "print(\"X_test_iris Shape: \", X_test_iris.shape)\n",
        "print(\"X_test_face Shape: \", X_test_face.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSrP0XYQZJV6"
      },
      "source": [
        "from keras import backend as K\n",
        "def recall(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
        "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
        "\n",
        "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return recall\n",
        "\n",
        "\n",
        "def precision(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
        "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
        "\n",
        "    # Precision = (True Positive) / (True Positive + False Positive)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return precision\n",
        "\n",
        "\n",
        "def f1score(y_target, y_pred):\n",
        "    _recall = recall(y_target, y_pred)\n",
        "    _precision = precision(y_target, y_pred)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
        "    \n",
        "    # return a single tensor value\n",
        "    return _f1score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv1-Ovqs7hGw"
      },
      "source": [
        "################### 홍채 모델 ###################\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D, Dropout, Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "\n",
        "# 기존 코드 수정\n",
        "def get_iris_model():\n",
        "    m1=Sequential()\n",
        "    m1.add(BatchNormalization(input_shape = (160,160,3)))\n",
        "    m1.add(Convolution2D(32, (3,3), activation ='relu', input_shape = (160, 160, 3))) \n",
        "    m1.add(MaxPooling2D(pool_size=2))\n",
        "    m1.add(Dropout(0.1))\n",
        "\n",
        "    m1.add(Convolution2D(filters=64,kernel_size=3,padding='same',activation='relu'))\n",
        "    m1.add(MaxPooling2D(pool_size=2))\n",
        "    m1.add(Dropout(0.1))\n",
        "\n",
        "    m1.add(Convolution2D(filters=128,kernel_size=3,padding='same',activation='relu'))\n",
        "    m1.add(MaxPooling2D(pool_size=2))\n",
        "    m1.add(Dropout(0.1))\n",
        "\n",
        "    m1.add(Convolution2D(filters=256,kernel_size=3,padding='same',activation='relu'))\n",
        "    m1.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "    m1.add(Flatten()) \n",
        "    m1.add(Dense(units=160,activation = 'relu'))\n",
        "    m1.add(BatchNormalization())\n",
        "    m1.add(Dense(units = 64, activation = 'softmax'))\n",
        "    m1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', precision, recall, f1score])  \n",
        "    return m1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnqeVvgTHGWZ"
      },
      "source": [
        "iris_model=get_iris_model()\n",
        "iris_model.summary()\n",
        "iris_model.fit(X_train_iris, y_train_iris, epochs=50, validation_data=(X_val_iris,y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5VMuTwvHc0I"
      },
      "source": [
        "################### 얼굴 모델 ###################\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D, Dropout, Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "def get_face_model():\n",
        "    m1=Sequential()\n",
        "    m1.add(BatchNormalization(input_shape = (160,160,3)))\n",
        "    m1.add(Convolution2D(32, (3,3), activation ='relu', input_shape = (160, 160, 3))) \n",
        "    m1.add(MaxPooling2D(pool_size=2))\n",
        "    m1.add(Convolution2D(filters=6,kernel_size=4,padding='same',activation='relu'))\n",
        "    m1.add(MaxPooling2D(pool_size=2))\n",
        "    m1.add(Convolution2D(filters=160,kernel_size=3,padding='same',activation='relu'))\n",
        "    m1.add(MaxPooling2D(pool_size=2))\n",
        "    m1.add(Convolution2D(filters=160,kernel_size=2,padding='same',activation='relu'))\n",
        "    m1.add(MaxPooling2D(pool_size=2))\n",
        "    m1.add(Flatten()) \n",
        "    m1.add(Dense(units=160,activation = 'relu'))\n",
        "    m1.add(Dense(units = 64, activation = 'softmax'))\n",
        "    m1.compile(optimizer='adam', loss = 'categorical_crossentropy',metrics=['accuracy', precision, recall, f1score])\n",
        "    return m1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTqVHMckHmaE"
      },
      "source": [
        "face_model=get_face_model()\n",
        "face_model.summary()\n",
        "face_model.fit(X_train_face, y_train_face, epochs=50, validation_data=(X_val_face,y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewzmP4znHSSx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLaDFGrVZK_D"
      },
      "source": [
        "predicted_iris = iris_model.predict(X_test_iris, batch_size=128)\n",
        "predicted_iris = np.argmax(np.round(predicted_iris),axis=1)\n",
        "predicted_iris = predicted_iris.tolist()\n",
        "\n",
        "print(\"Prediction :\",predicted_iris)\n",
        "print(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30vdPKX5Jmpz"
      },
      "source": [
        "predicted_face = face_model.predict(X_test_face, batch_size=128)\n",
        "predicted_face = np.argmax(np.round(predicted_face),axis=1)\n",
        "predicted_face = predicted_face.tolist()\n",
        "\n",
        "print(\"Prediction :\",predicted_face)\n",
        "print(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Suyz8JdcESz5"
      },
      "source": [
        "y_res_iris=[]\n",
        "y_res_iris = map(int, y_test)\n",
        "\n",
        "y_res_face=[]\n",
        "y_res_face = map(int, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp-NsoYeae7r"
      },
      "source": [
        "result_iris = { name:value for name, value in zip(y_res_iris, predicted_iris) }\n",
        "result_iris = sorted(result_iris.items())\n",
        "\n",
        "print(result_iris)\n",
        "\n",
        "result_face = { name:value for name, value in zip(y_res_face, predicted_face) }\n",
        "result_face = sorted(result_face.items())\n",
        "\n",
        "print(result_face)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QTh3s_waWIY"
      },
      "source": [
        "import csv\n",
        "with open('1871080_이혜승_멀티모달_1차_답안_모델2개.csv', 'w', newline='') as file:\n",
        "    mywriter = csv.writer(file, delimiter=',')\n",
        "    mywriter.writerows(result_iris)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}